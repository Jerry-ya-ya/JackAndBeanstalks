from flask import Blueprint, current_app
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime, timedelta
from routes.crawler.logic import fetch_and_store_news
import pytz
from flask import Flask
from models import ScheduleState, db

from routes.crawler.crawlerlogger import crawler_logger

last_run = None
next_run = None
scheduler = None
app = None

schedule_bp = Blueprint('schedule_bp', __name__)

def start_scheduler(flask_app: Flask):
    global next_run, scheduler, app
    app = flask_app # å°‡å¤–éƒ¨å‚³å…¥çš„ Flask æ‡‰ç”¨è¨­å®šçµ¦å…¨åŸŸè®Šæ•¸ appï¼Œè®“å…¶ä»–å‡½æ•¸èƒ½ç”¨å®ƒçš„ app_context
    if scheduler is None: # é¿å…é‡è¤‡å•Ÿå‹•æ’ç¨‹å™¨ï¼Œåªå•Ÿå‹•ä¸€æ¬¡ã€‚
        try:
            scheduler = BackgroundScheduler(timezone='Asia/Taipei') # è¨­å®šæ™‚å€ç‚ºå°åŒ—æ™‚é–“

            scheduler.add_job(scheduled_task, 'interval', minutes=15) # è¨­å®šæ’ç¨‹å™¨ï¼Œæ¯ 15 åˆ†é˜åŸ·è¡Œä¸€æ¬¡

            tz = pytz.timezone('Asia/Taipei')
            next_run = datetime.now(tz) + timedelta(minutes=15) #  è¨˜éŒ„ã€Œä¸‹ä¸€æ¬¡åŸ·è¡Œæ™‚é–“ã€è®Šæ•¸ï¼Œä¾›å‰ç«¯æŸ¥è©¢ç”¨

            scheduler.start()# å•Ÿå‹•æ’ç¨‹å™¨

            scheduled_task() # ç«‹å³åŸ·è¡Œä¸€æ¬¡ï¼Œå•Ÿå‹•å¾Œä¸ç”¨ç­‰ 15 åˆ†é˜
            crawler_logger.info("Scheduler started successfully")
        except Exception as e:
            crawler_logger.info(f"Error starting scheduler: {e}")

def scheduled_task():
    global last_run, next_run
    crawler_logger.info("ğŸŸ¡ scheduled_task è¢«å‘¼å«")
    try: # Flask çš„è³‡æ–™åº«æ“ä½œéœ€è¦æœ‰ã€Œæ‡‰ç”¨ä¸Šä¸‹æ–‡ã€ï¼Œé€™å¥æ˜¯å¿…è¦çš„åŒ…è£ï¼
        with app.app_context():
            added = fetch_and_store_news() # åŸ·è¡Œä½ è‡ªå®šç¾©çš„çˆ¬èŸ²é‚è¼¯ï¼Œä¸¦å›å‚³æ–°å¢äº†å¹¾ç­†è³‡æ–™

            tz = pytz.timezone('Asia/Taipei')
            now = datetime.now(tz)
            future = now + timedelta(minutes=15) # è¨˜éŒ„ä¸Šæ¬¡èˆ‡ä¸‹æ¬¡åŸ·è¡Œæ™‚é–“ï¼ˆçµ¦å‰ç«¯ info é¡¯ç¤ºï¼‰

            state = ScheduleState.query.filter_by(job_name="news_crawler").first()
            state.last_run = now
            state.next_run = future
            db.session.commit()

            crawler_logger.info(f"ğŸŸ¢ çˆ¬èŸ²æˆåŠŸæ–°å¢ {added} ç­†è³‡æ–™")
    except Exception as e:
        crawler_logger.info(f"ğŸ”´ çˆ¬èŸ²æ’ç¨‹éŒ¯èª¤: {e}")